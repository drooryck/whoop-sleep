---
title: "Final Project - Group 2"
subtitle: "Sam, Eva, Dries, Caden, Jonas"
output: pdf_document
date: "2024-12-12"
---

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r}
suppressMessages({
  library(ggplot2)
  library(reshape2)
  library(dplyr)
  library(faraway)
  library(lubridate)
})
```

# 1 - Exploratory Data Analysis

[COMMENT: This is where we did analysis on WHOOP Strain metric to test for collineaity with the rest of our data.]: #

------------------------------------------------------------------------

### 1.11 - Testing WHOOP’s Activity Strain Metric - LINEAR REGRESSION

```{r activity strain}
workouts <- read.csv("data/workouts.csv")

workouts_1 <- workouts %>%
         select(-`GPS.enabled`, 
         -`Distance..meters.`, 
         -`Altitude.gain..meters.`, 
         -`Altitude.change..meters.`,
         - `Cycle.start.time`,
         - `Cycle.end.time`,
         - `Cycle.timezone`,
         - `Workout.start.time`,
         - `Workout.end.time`,
         - `Activity.name`)

activity_strain <- lm(Activity.Strain ~ ., data = workouts_1)
sumary(activity_strain)
summary(activity_strain)$adj.r.squared
```

\textbf{Conclusion:} 95% of the variability in `Activity Strain` is explained by our reduced dataset and a very simple model. This may gives us reason to drop this variable as well.

### 1.12 - Testing WHOOP’s Activity Strain Metric - CORRELATION PLOT

```{r}
cor_matrix <- cor(workouts_1, use = "complete.obs")
melted_cor_matrix <- melt(cor_matrix)

ggplot(melted_cor_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = c("blue", "white", "red"), values = c(0, 0.5, 1), limit = c(-1, 1)) +
  theme_light(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(size = 12)) +
  labs(title = "Correlation Matrix Heatmap", x = "Variables", y = "Variables") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"))


threshold <- 0.85
high_corr_pairs <- which(abs(cor_matrix) > threshold, arr.ind = TRUE)
high_corr_df <- data.frame(
  Column1 = rownames(cor_matrix)[high_corr_pairs[, 1]],
  Column2 = colnames(cor_matrix)[high_corr_pairs[, 2]],
  Correlation = cor_matrix[high_corr_pairs]
)
high_corr_df <- high_corr_df[high_corr_df$Column1 != high_corr_df$Column2, ]
print(high_corr_df)
```

\textbf{Conclusion:} to avoid collinearity between `Energy Burned` and `Activity Strain`, we will drop the `Activity Strain` column since `Energy Burned` provides an empirical datapoint.



\textbf{Conclusion:} to avoid collinearity between `Energy Burned` and `Activity Strain`, we will drop the `Activity Strain` column since `Energy Burned` provides an empirical datapoint.

------------------------------------------------------------------------

# 2 - Data Prep, Preparing the Explanatory Variables (Workouts)

------------------------------------------------------------------------

### 2.1 - daily_workouts

[COMMENT: This is the data that is accounted for by workouts.csv]: #

```{r}
##note: premise of time series = One Day One Observation

# Step 1: Drop the columns with no data
daily_workouts <- workouts %>%
  select(-c("GPS.enabled", "Distance..meters.", 
            "Altitude.gain..meters.", "Altitude.change..meters."))

# Step 2: Remove the useless or redundant data
daily_workouts <- daily_workouts %>%
  select(-c("Cycle.start.time", "Cycle.end.time", "Cycle.timezone"))

# Step 3: Convert Activity.name to a factor
# Note: There was a level in the factor called "Other" with 1 observation,
# which was removed for clarity, since "Other" is no interpretable.
# Note: We also removed the activity "Activity" also for lack of interpretability.
# This activity had 65 observations.
daily_workouts$Activity.name <- as.factor(daily_workouts$Activity.name)
daily_workouts <- daily_workouts %>%
  mutate(
    Assault_Bike = Activity.name == "Assault Bike",
    Cycling = Activity.name == "Cycling",
    Elliptical = Activity.name == "Elliptical",
    Functional_Fitness = Activity.name == "Functional Fitness",
    Hiking_Rucking = Activity.name == "Hiking/Rucking",
    Powerlifting = Activity.name == "Powerlifting",
    Rowing = Activity.name == "Rowing",
    Running = Activity.name == "Running",
    Spin = Activity.name == "Spin",
    Stairmaster = Activity.name == "Stairmaster",
    Walking = Activity.name == "Walking",
    Weightlifting = Activity.name == "Weightlifting",
    Yoga = Activity.name == "Yoga"
  ) %>%
  select(-Activity.name)  # Remove the original Activity.name column

# Step 4: Convert 'Workout.start.time' to Date and extract the date portion
daily_workouts <- daily_workouts %>%
  mutate(Workout.start.date = as.Date(Workout.start.time, format = "%Y-%m-%d %H:%M:%S"))

# Step 5: Group by date and summarize
daily_workouts <- daily_workouts %>%
  group_by(Workout.start.date) %>%
  summarise(
    # Sum of 'Duration..min.' and 'Energy.burned..cal.'
    Total.Duration.min = sum(`Duration..min.`, na.rm = TRUE),
    Total.Energy.burned.cal = sum(`Energy.burned..cal.`, na.rm = TRUE),
    
    # Weighted average time spent at average heart rate
    Weighted.Avg.HR = sum(`Average.HR..bpm.` * `Duration..min.`, na.rm = TRUE) / 
      sum(`Duration..min.`, na.rm = TRUE),
    
    # Weighted average time spent for heart rate zones
    Weighted.Avg.HR.Zone.1 = sum(`HR.Zone.1..` * `Duration..min.`, na.rm = TRUE) / 
      sum(`Duration..min.`, na.rm = TRUE),
    Weighted.Avg.HR.Zone.2 = sum(`HR.Zone.2..` * `Duration..min.`, na.rm = TRUE) / 
      sum(`Duration..min.`, na.rm = TRUE),
    Weighted.Avg.HR.Zone.3 = sum(`HR.Zone.3..` * `Duration..min.`, na.rm = TRUE) / 
      sum(`Duration..min.`, na.rm = TRUE),
    Weighted.Avg.HR.Zone.4 = sum(`HR.Zone.4..` * `Duration..min.`, na.rm = TRUE) / 
      sum(`Duration..min.`, na.rm = TRUE),
    Weighted.Avg.HR.Zone.5 = sum(`HR.Zone.5..` * `Duration..min.`, na.rm = TRUE) / 
      sum(`Duration..min.`, na.rm = TRUE),
    
    # Keep the Max HR for the day
    Max.HR.bpm = max(`Max.HR..bpm.`, na.rm = TRUE),
    
    # Sum of boolean values for each activity (to count the occurrences per day)
    Total.Assault_Bike = sum(Assault_Bike, na.rm = TRUE),
    Total.Cycling = sum(Cycling, na.rm = TRUE),
    Total.Elliptical = sum(Elliptical, na.rm = TRUE),
    Total.Functional_Fitness = sum(Functional_Fitness, na.rm = TRUE),
    Total.Hiking_Rucking = sum(Hiking_Rucking, na.rm = TRUE),
    Total.Powerlifting = sum(Powerlifting, na.rm = TRUE),
    Total.Rowing = sum(Rowing, na.rm = TRUE),
    Total.Running = sum(Running, na.rm = TRUE),
    Total.Spin = sum(Spin, na.rm = TRUE),
    Total.Stairmaster = sum(Stairmaster, na.rm = TRUE),
    Total.Walking = sum(Walking, na.rm = TRUE),
    Total.Weightlifting = sum(Weightlifting, na.rm = TRUE),
    Total.Yoga = sum(Yoga, na.rm = TRUE)) %>%
  
  #Create a new column for heart zone 0
  mutate(Weighted.Avg.HR.Zone.0 = round((100 - Weighted.Avg.HR.Zone.1 - Weighted.Avg.HR.Zone.2 - 
           Weighted.Avg.HR.Zone.3 - Weighted.Avg.HR.Zone.4 - Weighted.Avg.HR.Zone.5), 2))
```

### 2.2 - missing_dates_df

[COMMENT: This adds the rest days (aka, the data that is accounted for by workouts.csv)]: #


```{r}
# Step 1: Identify the date range (min and max dates)
date_range <- seq.Date(min(daily_workouts$Workout.start.date),
                       max(daily_workouts$Workout.start.date), by = "day")

# Step 2: Create a data frame for all dates in the range
missing_dates_df <- data.frame(
  Workout.start.date = date_range,
  Total.Duration.min = 0,
  Total.Energy.burned.cal = 0,
  Weighted.Avg.HR = 57,  # Set resting heart rate
  Weighted.Avg.HR.Zone.1 = 0,
  Weighted.Avg.HR.Zone.2 = 0,
  Weighted.Avg.HR.Zone.3 = 0,
  Weighted.Avg.HR.Zone.4 = 0,
  Weighted.Avg.HR.Zone.5 = 0,
  Max.HR.bpm = 57,  # Set resting heart rate for Max HR
  Total.Assault_Bike = 0,
  Total.Cycling = 0,
  Total.Elliptical = 0,
  Total.Functional_Fitness = 0,
  Total.Hiking_Rucking = 0,
  Total.Powerlifting = 0,
  Total.Rowing = 0,
  Total.Running = 0,
  Total.Spin = 0,
  Total.Stairmaster = 0,
  Total.Walking = 0,
  Total.Weightlifting = 0,
  Total.Yoga = 0,
  Weighted.Avg.HR.Zone.0 = 100  # Set HR Zone 0 to 100
)
```

### 2.3 - Combining daily_workouts and missing_dates_df into One Continuous Exploratory Dataset


```{r}
# Step 3: Combine with the original data
daily_workouts <- bind_rows(
  daily_workouts,
  missing_dates_df
)

# Step 4: Remove any duplicates and sort by date
daily_workouts <- daily_workouts %>%
  distinct(Workout.start.date, .keep_all = TRUE) %>%
  arrange(Workout.start.date)

```

\textbf{Columns in wrangled workouts dataset:}

-   Workout.start.date
-   Total.Duration.min
-   Total.Energy.burned.cal
-   Weighted.Avg.HR
-   Weighted.Avg.HR.Zone.0
-   Weighted.Avg.HR.Zone.1
-   Weighted.Avg.HR.Zone.2
-   Weighted.Avg.HR.Zone.3
-   Weighted.Avg.HR.Zone.4
-   Weighted.Avg.HR.Zone.5
-   Max.HR.bpm
-   Total.Assault_Bike
-   Total.Cycling
-   Total.Elliptical
-   Total.Functional_Fitness
-   Total.Hiking_Rucking
-   Total.Powerlifting
-   Total.Rowing
-   Total.Running
-   Total.Spin
-   Total.Stairmaster
-   Total.Walking
-   Total.Weightlifting
-   Total.Yoga

------------------------------------------------------------------------

\clearpage

# 3 - Data Prep, Preparing the Response Variable: Sleep 

\textbf{Columns in sleep dataset:}

-   Cycle.start.time
-   Cycle.end.time
-   Cycle.timezone
-   Sleep.onset
-   Wake.onset
-   Sleep.performance..
-   Respiratory.rate..rpm.
-   Asleep.duration..min.
-   In.bed.duration..min.
-   Light.sleep.duration..min.
-   Deep..SWS..duration..min.
-   REM.duration..min.
-   Awake.duration..min.
-   Sleep.need..min.
-   Sleep.debt..min.
-   Sleep.efficiency..
-   Sleep.consistency..
-   Nap

### 3.1 - Sleep CORRELATION PLOT

```{r}
# Load necessary libraries
library(ggplot2)
library(reshape2)

# Read the CSV file
sleeps <- read.csv("data/sleeps.csv")

# Select only numeric columns
sleeps_numeric <- sleeps[sapply(sleeps, is.numeric)]

# Compute the correlation matrix using complete observations
cor_matrix2 <- cor(sleeps_numeric, use = "complete.obs")

# Melt the correlation matrix for visualization
melted_cor_matrix2 <- melt(cor_matrix2)

# Create the heatmap using ggplot2
ggplot(melted_cor_matrix2, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = c("blue", "white", "red"), values = c(0, 0.5, 1), limit = c(-1, 1)) +
  theme_light(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(size = 12)) +
  labs(title = "Correlation Matrix Heatmap", x = "Variables", y = "Variables") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"))

# Define the correlation threshold
threshold <- 0.85

# Find pairs of variables with correlations exceeding the threshold
high_corr_pairs2 <- which(abs(cor_matrix2) > threshold, arr.ind = TRUE)

# Create a data frame of the high-correlation pairs
high_corr_df2 <- data.frame(
  Column1 = rownames(cor_matrix2)[high_corr_pairs2[, 1]],
  Column2 = colnames(cor_matrix2)[high_corr_pairs2[, 2]],
  Correlation = cor_matrix2[high_corr_pairs2]
)

# Remove self-correlations (diagonal elements)
high_corr_df2 <- high_corr_df2[high_corr_df2$Column1 != high_corr_df2$Column2, ]

# Print the high-correlation pairs
print(high_corr_df2)


```

```{r}

#Step 1: remove unnecessary or redundant variables
daily_sleep <- sleeps %>%
  select(- `Cycle.start.time`,
         - `Cycle.end.time`,
         - `Cycle.timezone`,
         - `Sleep.performance..`,
         - `In.bed.duration..min.`,
         - `Sleep.debt..min.`,
         - `Sleep.efficiency..`,
         - `Nap`)

# Step 2: Group by Wake.date and calculate the number of wake-ups and total sleep time
nap_data <- daily_sleep %>%
  mutate(Wake.date = as.Date(Wake.onset)) %>%
  group_by(Wake.date) %>%
  summarise(
    Nap.count = n_distinct(Wake.onset) - 1,
    Nap.duration = sum(Asleep.duration..min., na.rm = TRUE) - max(Asleep.duration..min.)
  ) %>%
  ungroup() %>%
  mutate(Nap.duration = ifelse(Nap.count == 0, 0, Nap.duration))

# Step 3: Group by Wake.onset date and select the row with the largest Asleep.duration..min.
sleep_data <- daily_sleep %>%
  # Group by Wake.onset
  group_by(as.Date(Wake.onset)) %>%
  # Filter to keep the row with the max Asleep.duration..min.
  filter(Asleep.duration..min. == max(Asleep.duration..min.)) %>%  
  slice(1) %>% ungroup()

#Step 4: We will think of your sleep as the day that you worked out
sleep_data <- sleep_data %>%
  mutate(Sleep.date = as.Date(Wake.onset) - days(1))

# Step 5: Merge sleep_data and nap_data by their respective date columns
sleep_nap_data <- sleep_data %>%
  left_join(nap_data, by = c("Sleep.date" = "Wake.date")) %>%
  filter(!is.na(Nap.count))
```

------------------------------------------------------------------------

\clearpage

# 4 - Final Dataset, Merge Sleep and Workout Data

[COMMENT: v0 Merged Dataset]: #

```{r}
merged_data <- sleep_nap_data %>%
  inner_join(daily_workouts, by = c("Sleep.date" = "Workout.start.date")) %>%
  select(-`Sleep.onset`,
         - `Wake.onset`,
         - `as.Date(Wake.onset)`)

str(merged_data)
```

[COMMENT: v1 merged dataset]: #
```{r}
# Load necessary libraries
library(dplyr)

# Select the specified variables from merged_data_1
merged_data_1 <- merged_data %>%
  select(
    -Sleep.date,
    -Total.Assault_Bike,
    -Total.Cycling,
    -Total.Elliptical,
    -Total.Functional_Fitness,
    -Total.Hiking_Rucking,
    -Total.Powerlifting,
    -Total.Rowing,
    -Total.Running,
    -Total.Spin,
    -Total.Stairmaster,
    -Total.Walking,
    -Total.Weightlifting,
    -Total.Yoga
  )

```

# 5 - Baseline Model

```{r}
# Remove rows with missing values
cleaned_data <- na.omit(merged_data_1)

# Select only numeric columns
cleaned_numeric <- cleaned_data[sapply(cleaned_data, is.numeric)]

# Compute the correlation matrix using complete observations
cor_matrix3 <- cor(cleaned_numeric, use = "complete.obs")

# Melt the correlation matrix for visualization
melted_cor_matrix3 <- melt(cor_matrix3)

ggplot(melted_cor_matrix3, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colors = c("blue", "white", "red"), values = c(0, 0.5, 1), limit = c(-1, 1)) +
  theme_light(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(size = 12)) +
  labs(title = "Correlation Matrix Heatmap", x = "Variables", y = "Variables") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"))

# Define the correlation threshold
threshold <- 0.85

high_corr_pairs3 <- which(abs(cor_matrix3) > threshold, arr.ind = TRUE)
high_corr_df3 <- data.frame(
  Column1 = rownames(cor_matrix3)[high_corr_pairs3[, 1]],
  Column2 = colnames(cor_matrix3)[high_corr_pairs3[, 2]],
  Correlation = cor_matrix3[high_corr_pairs3]
)
high_corr_df3 <- high_corr_df3[high_corr_df3$Column1 != high_corr_df3$Column2, ]

print(high_corr_df3)
```


```{r}
# Basic model, no interaction terms
model <- lm(Deep..SWS..duration..min./Asleep.duration..min. ~ ., data = cleaned_data)
summary(model)$adj.r.squared
plot(model, 1)
plot(model, 2)

# Model with Interactive terms (note this actually has a stronger R^2 than the highly complex model)
model_rr <- lm(Deep..SWS..duration..min./Asleep.duration..min. ~ (Total.Energy.burned.cal)*(.), data = cleaned_data)
summary(model_rr)$adj.r.squared
plot(model_rr, 1)
plot(model_rr, 2)

# highly complex model
model_full <- lm(Deep..SWS..duration..min./Asleep.duration..min. ~ (.)^2, data = cleaned_data)
summary(model_full)$adj.r.squared
plot(model_full, 1)
plot(model_full, 2)
```

# 6 - Model Selection

```{r, results = "hide"}
#Originally the model_full was not running on posit - we examined the dataset again and were able to take out more unnecesary values. As a result, we could run step selection on the full model and this gave us a better lowest AIC value (4128.74) than using the interactive term (4197.64)

## Results with Interactive term were:
# Start:  AIC=-4415.95)

## Results with full model term were: 
# Start:  AIC=-4295.39)


# Remove rows with missing values
cleaned_data <- na.omit(merged_data_1)

model_backward <- step(model_rr, direction = "backward")
# Intercept-only model with the cleaned data
model_intercept <- lm(Deep..SWS..duration..min./Asleep.duration..min. ~ 1, data = cleaned_data)
model_forward <- step(model_intercept, direction = "forward", scope = formula(model_rr))

```

```{r}
#Best Adj-R^2
formula(model_backward)
summary(model_backward)$adj.r.squared
plot(model_backward, 1)
plot(model_backward, 2)

formula(model_forward)
summary(model_forward)$adj.r.squared
plot(model_forward, 1)
plot(model_forward, 2)
```

# 7 - Assumption Checking for Linear Models

```{r}
library(ggplot2)

predictors <- merged_data_1 %>%
  select(-Deep..SWS..duration..min.)  

for (predictor in names(predictors)) {
  print(
    ggplot(data = predictors, aes_string(x = predictor)) +
      geom_histogram(fill = "skyblue", color = "black", bins = 30, alpha = 0.8) +
      theme_minimal() +
      labs(title = paste("Distribution of", predictor),
           x = predictor, y = "Count") +
      theme(
        plot.title = element_text(size = 14, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10)
      )
  )
}
```

```{r}
# 1. Linearity and Homoscedasticity - Residuals vs Fitted
par(mfrow = c(1, 2))  # Split screen into 2 plots
plot(model_backward, which = 1)  # Residuals vs Fitted
plot(model_backward, which = 2)  # Q-Q plot for residuals

# Extract the model's data for consistent lengths
model_data <- model.frame(model_backward)
residuals_backward <- residuals(model_backward)

# 2. Independence - Residuals over Total Duration
plot(model_data$Total.Duration.min, residuals_backward,
     main = "Residuals vs Total Duration",
     xlab = "Total Duration (min)", ylab = "Residuals",
     col = "darkblue", pch = 20)

# 3. Residual Histogram
hist(residuals_backward, breaks = 30, col = "skyblue",
     main = "Histogram of Residuals", xlab = "Residuals")
```

# 8 - Prediction

## 8.1 - Baseline

```{r}
# Why? A simple model to establish a baseline for predictive performance.
# Linear regression helps identify if the relationship between predictors and response is linear. We could not use a Logistic Regression model because our output is not binary

library("Metrics")
set.seed(139)  
split <- sample(1:nrow(merged_data_1), 0.7 * nrow(merged_data_1))  # 70/30 split

train <- merged_data_1[split, ]
test <- merged_data_1[-split, ]

# Baseline Linear Regression
baseline_model <- lm(Deep..SWS..duration..min. / Asleep.duration..min. ~ ., data = train)
baseline_predictions <- predict(baseline_model, test)

# Model evaluation (RMSE and R-squared)
library(Metrics)
rmse_baseline <- rmse(test$Deep..SWS..duration..min. / test$Asleep.duration..min., baseline_predictions)
r2_baseline <- cor(test$Deep..SWS..duration..min. / test$Asleep.duration..min., baseline_predictions)^2

print(rmse_baseline)
print(r2_baseline)
```

## 8.2 - LASSO

```{r}
# Lasso Regression
# Why? Useful for datasets with many predictors to automatically perform feature selection.
# Lasso helps reduce overfitting and simplifies the model by shrinking less important coefficients to zero.

set.seed(139)
library(glmnet)
library(Metrics)

train_clean <- na.omit(train)
test_clean <- na.omit(test)

x_train <- as.matrix(train_clean %>% select(-Deep..SWS..duration..min.))
y_train <- train_clean$Deep..SWS..duration..min. / train_clean$Asleep.duration..min.

x_test <- as.matrix(test_clean %>% select(-Deep..SWS..duration..min.))
y_test <- test_clean$Deep..SWS..duration..min. / test_clean$Asleep.duration..min.

# Fit Lasso model
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1)
best_lambda <- lasso_model$lambda.min

# Predict on test set
lasso_preds <- predict(lasso_model, s = best_lambda, newx = x_test)

# Calculate RMSE
rmse_lasso <- rmse(y_test, lasso_preds)

# Calculate R-squared
rss <- sum((y_test - lasso_preds)^2)  
tss <- sum((y_test - mean(y_test))^2)  
r2_lasso <- 1 - (rss / tss)

print(paste("Lasso RMSE:", round(rmse_lasso, 4)))
print(paste("Lasso R-squared:", round(r2_lasso, 4)))
```

## 8.3 - Random Forest

```{r}
# Random Forest
# Why? Suitable for capturing nonlinear relationships and interactions between predictors.
# Random Forest is also robust to overfitting and can rank feature importance.

library(caret)
library(Metrics)
library(randomForest)

set.seed(139)
train_clean <- na.omit(train)  
test_clean <- na.omit(test)

# Define the response variable
y_train <- train_clean$Deep..SWS..duration..min. / train_clean$Asleep.duration..min.
y_test <- test_clean$Deep..SWS..duration..min. / test_clean$Asleep.duration..min.

x_train <- train_clean %>% select(-Deep..SWS..duration..min.)
x_test <- test_clean %>% select(-Deep..SWS..duration..min.)

# Combine predictors and response for training
train_data <- cbind(x_train, y_train)

# Define cross-validation settings
set.seed(139)
cv_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train Random Forest model with cross-validation
rf_cv_model <- train(
  x = x_train, 
  y = y_train,
  method = "rf",
  trControl = cv_control,
  tuneGrid = expand.grid(mtry = seq(2, sqrt(ncol(x_train)), length.out = 5)),  # Adjust mtry values
  ntree = 1000
)

# Print the best hyperparameters
print(rf_cv_model$bestTune)

# Predict on test set using the best model
rf_cv_preds <- predict(rf_cv_model, x_test)

# Evaluate the model: RMSE and R-squared
rmse_rf_cv <- rmse(y_test, rf_cv_preds)
rss_rf_cv <- sum((y_test - rf_cv_preds)^2)
tss_rf_cv <- sum((y_test - mean(y_test))^2)
r2_rf_cv <- 1 - (rss_rf_cv / tss_rf_cv)

print(paste("Cross-Validated Random Forest RMSE:", round(rmse_rf_cv, 4)))
print(paste("Cross-Validated Random Forest R-squared:", round(r2_rf_cv, 4)))

```